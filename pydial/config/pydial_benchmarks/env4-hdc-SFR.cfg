# Error model: 15% error rate, DSTC2 confscorer, DSTC2 nbestgenerator
# User model: standard sampled params, sampled patience
# Masks: off

###### General parameters ######
[GENERAL]
domains = SFRestaurants
singledomain = True 
tracedialog = 0
seed = 07051991

[exec_config]
configdir = _benchmarkpolicies
logfiledir = _benchmarklogs
numtrainbatches = 4
traindialogsperbatch = 1000
numbatchtestdialogs =  500
trainsourceiteration = 0
numtestdialogs =  500
trainerrorrate = 15
testerrorrate  = 15
testeverybatch = True
#deleteprevpolicy = True

[logging]
usecolor = False
screen_level = results
file_level = results
file = auto

###### Environment parameters ######

[agent]
maxturns = 25

[usermodel]
usenewgoalscenarios = True
oldstylepatience = False
patience = 4,6
configfile = config/sampledUM.cfg

[errormodel]
nbestsize = 5
confusionmodel = LevenshteinConfusions
nbestgeneratormodel = DSTC2NBestGenerator
confscorer = DSTC2
configfile = config/set1-ErrorModel.cfg


[summaryacts]
maxinformslots = 5
informmask = False
requestmask = False
informcountaccepted = 4
byemask = False

###### Dialogue Manager parameters ######

## Comment the following lines if using any other policy (this uses handcrafted policy)##
[policy]
policydir = _benchmarkpolicies
belieftype = focus
useconfreq = False
learning = True
policytype = hdc
startwithhello = False
inpolicyfile = auto
outpolicyfile = auto

## Uncomment for GP policy ##
#[policy]
#policydir = _benchmarkpolicies
#belieftype = focus
#useconfreq = False
#learning = True
#policytype = gp
#startwithhello = False
#inpolicyfile = auto
#outpolicyfile = auto
#
#[gppolicy]
#kernel = polysort
#
#[gpsarsa]
#random = False
#scale = 3

## Uncomment for DQN policy ##
#[policy]
#policydir = _benchmarkpolicies
#belieftype = focus
#useconfreq = False
#learning = True
#policytype = dqn
#startwithhello = False
#inpolicyfile = auto
#outpolicyfile = auto

#[dqnpolicy]
#maxiter = 4000
#gamma = 0.99
#learning_rate = 0.001
#tau = 0.02
#replay_type = vanilla
#minibatch_size = 64
#capacity = 6000
#exploration_type = e-greedy
#episodeNum= 0.0
#epsilon_start = 0.3
#epsilon_end = 0.0
#n_in = 636
#features = ["discourseAct", "method", "requested", "full", "lastActionInformNone", "offerHappened", "inform_info"]
#max_k = 5
#learning_algorithm = dqn
#architecture = vanilla
#h1_size = 300
#h2_size = 100
#training_frequency = 2
#n_samples = 1
#stddev_var_mu = 0.01
#stddev_var_logsigma = 0.01
#mean_log_sigma = 0.000001
#sigma_prior = 1.5
#alpha =0.85
#alpha_divergence =False
#sigma_eps = 0.01
#delta = 1.0
#beta = 0.95
#is_threshold = 5.0
#train_iters_per_episode = 1

## Uncomment for A2C policy ##
#[policy]
#policydir = _benchmarkpolicies
#belieftype = focus
#useconfreq = False
#learning = True
#policytype = a2c
#startwithhello = False
#inpolicyfile = auto
#outpolicyfile = auto

#[dqnpolicy]
#maxiter = 4000
#gamma = 0.99
#learning_rate = 0.001
#tau = 0.02
#replay_type = vanilla
#minibatch_size = 64
#capacity = 1000
#exploration_type = e-greedy
#episodeNum= 0.0
#epsilon_start = 0.5
#epsilon_end = 0.0
#n_in = 636
#features = ["discourseAct", "method", "requested", "full", "lastActionInformNone", "offerHappened", "inform_info"]
#max_k = 5
#learning_algorithm = dqn
#architecture = vanilla
#h1_size = 200
#h2_size = 75
#training_frequency = 2
#n_samples = 1
#stddev_var_mu = 0.01
#stddev_var_logsigma = 0.01
#mean_log_sigma = 0.000001
#sigma_prior = 1.5
#alpha =0.85
#alpha_divergence =False
#sigma_eps = 0.01
#delta = 1.0
#beta = 0.95
#is_threshold = 5.0
#train_iters_per_episode = 1

## Uncomment for eNAC policy ##
#[policy]
#policydir = _benchmarkpolicies
#belieftype = focus
#useconfreq = False
#learning = True
#policytype = enac
#startwithhello = False
#inpolicyfile = auto
#outpolicyfile = auto

#[dqnpolicy]
#maxiter = 4000
#gamma = 0.99
#learning_rate = 0.001
#tau = 0.02
#replay_type = vanilla
#minibatch_size = 64
#capacity = 1000
#exploration_type = e-greedy
#episodeNum= 0.0
#epsilon_start = 0.3
#epsilon_end = 0.0
#n_in = 636
#features = ["discourseAct", "method", "requested", "full", "lastActionInformNone", "offerHappened", "inform_info"]
#max_k = 5
#learning_algorithm = dqn
#architecture = vanilla
#h1_size = 130
#h2_size = 50
#training_frequency = 2
#n_samples = 1
#stddev_var_mu = 0.01
#stddev_var_logsigma = 0.01
#mean_log_sigma = 0.000001
#sigma_prior = 1.5
#alpha =0.85
#alpha_divergence =False
#sigma_eps = 0.01
#delta = 1.0
#beta = 0.95
#is_threshold = 5.0
#train_iters_per_episode = 1

###### Evaluation parameters ######

[eval]
rewardvenuerecommended=0
penaliseallturns = True
wrongvenuepenalty = 0
notmentionedvaluepenalty = 0      
successmeasure = objective 
successreward = 20

